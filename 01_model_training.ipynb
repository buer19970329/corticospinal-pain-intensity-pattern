{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091069a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f628611",
   "metadata": {},
   "source": [
    "# CsPIP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94776927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "datadir = '0data'\n",
    "train_y = np.load(os.path.join(datadir,'d1_train_y.npy'))\n",
    "test_y = np.load(os.path.join(datadir,'d1_test_y.npy'))\n",
    "train_pred = np.load(os.path.join(datadir,'d1_train_pred.npy'))\n",
    "test_pred = np.load(os.path.join(datadir,'d1_test_pred.npy'))\n",
    "train_set = np.load(os.path.join(datadir,'d1_train_set.npy'))\n",
    "test_set = np.load(os.path.join(datadir,'d1_test_set.npy'))\n",
    "train_groups = np.load(os.path.join(datadir,'d1_train_group.npy'))\n",
    "qh_lp_copes = np.load(os.path.join(datadir,'d2_qh_lp_set.npy'))\n",
    "qh_hp_copes = np.load(os.path.join(datadir,'d2_qh_hp_set.npy'))\n",
    "qh_lp_pred = np.load(os.path.join(datadir,'d2_qh_lp_pred.npy'))\n",
    "qh_hp_pred = np.load(os.path.join(datadir,'d2_qh_hp_pred.npy'))\n",
    "qh_lp_y = np.load(os.path.join(datadir,'d2_qh_lp_y.npy'))\n",
    "qh_hp_y = np.load(os.path.join(datadir,'d2_qh_hp_y.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74060dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35521490257490257 1.004722370038097 0.1120164277933402\n",
      "PearsonRResult(statistic=np.float64(0.4670142128674764), pvalue=np.float64(0.00012999202320807405))\n",
      "--------------------\n",
      "PearsonRResult(statistic=np.float64(0.3382695270965094), pvalue=np.float64(0.03517839795001519))\n",
      "PearsonRResult(statistic=np.float64(0.5168412628472525), pvalue=np.float64(0.0007550686759007282))\n"
     ]
    }
   ],
   "source": [
    "lassopcr = Pipeline(steps=[\n",
    "                           ('scaler', StandardScaler()),\n",
    "                           ('pca', PCA()),\n",
    "                           ('lasso', Lasso())])\n",
    "\n",
    "# Set up the cross_valdation\n",
    "\n",
    "cv = GroupKFold(n_splits=10)\n",
    "\n",
    "# 并行得到逐样本的交叉验证预测\n",
    "y_pred = cross_val_predict(\n",
    "    lassopcr,            # 你的 Pipeline\n",
    "    X=train_set,\n",
    "    y=train_y,\n",
    "    groups=train_groups,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,           # -1 = 用完所有 CPU 核\n",
    "    method=\"predict\"     # 调用 predict\n",
    ")\n",
    "\n",
    "# 计算总体指标\n",
    "r      = pearsonr(train_y, y_pred)[0]\n",
    "rmse   = np.sqrt(mean_squared_error(train_y, y_pred))\n",
    "r2     = r2_score(train_y, y_pred)\n",
    "print(r, rmse, r2)\n",
    "lassopcr.fit(train_set, train_y)\n",
    "print(pearsonr(test_y, lassopcr.predict(test_set)))\n",
    "print('-'*20)\n",
    "print(pearsonr(qh_lp_y,lassopcr.predict(qh_lp_copes)))\n",
    "print(pearsonr(qh_hp_y,lassopcr.predict(qh_hp_copes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ed2ec",
   "metadata": {},
   "source": [
    "# bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbootstraps = 10000\n",
    "nstop = 200 # frequency to stop/save bootstraps\n",
    "njobs = -1\n",
    "brain_v = len_brain\n",
    "spial_v = len_spinal\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "outpath = '/mnt/lxm2/2025/03_Tens/lassopcr_res_0503_2a/boots_new'\n",
    "name = 'lassopcr'\n",
    "os.makedirs(opj(outpath, 'permsamples'), exist_ok=True)\n",
    "os.makedirs(opj(outpath, 'bootsamples'), exist_ok=True)\n",
    "def bootstrap_weights(X, Y,rs):\n",
    "    # Randomly select observations\n",
    "    rng = np.random.RandomState(rs)\n",
    "\n",
    "    boot_ids = rng.choice(np.arange(len(X)),\n",
    "                                size=len(X),\n",
    "                                replace=True)\n",
    "    try:\n",
    "        # Fit the classifier on this sample and get the weights\n",
    "        lassopcr.fit(X[boot_ids], Y[boot_ids])\n",
    "\n",
    "        # Return the weights and stats\n",
    "        return np.dot(lassopcr['pca'].components_.T, lassopcr['lasso'].coef_)\n",
    "    except:\n",
    "        print(\"SVD failed on a bootstrap sample. Skipping...\")\n",
    "        return rs  # 返回 None 而不是零向量\n",
    "# Run in parrallel and stop/save regurarly to run in multiple\n",
    "for i in tqdm(range(nbootstraps//nstop)):\n",
    "    # Check if file alrady exist in case bootstrap done x times\n",
    "    outbootfile = ['bootsamples_' + str(nstop) + 'samples_'\n",
    "                    + str(i+1) + '_of_'\n",
    "                    + str(nbootstraps//nstop) + '.npy']\n",
    "    print(\"Running permloop \" + str(i + 1) + ' out of ' + str(nbootstraps//nstop))\n",
    "    if not os.path.exists(opj(outpath, 'bootsamples', outbootfile[0])):\n",
    "        bootstrapped = Parallel(n_jobs=40,\n",
    "                                verbose=0)(delayed(bootstrap_weights)(X=train_set, Y=train_y,rs=i*200+ii)\n",
    "                                           for ii in range(nstop))\n",
    "        try:\n",
    "            bootstrapped = np.stack(bootstrapped)\n",
    "        except:\n",
    "            for ind, vv in enumerate(bootstrapped):\n",
    "                if isinstance(vv, int):  # Check if vv is of type int\n",
    "                    # Replace vv with the result of bootstrap_weights\n",
    "                    bootstrapped[ind] = bootstrap_weights(all_copes, all_y, vv)\n",
    "        bootstrapped = np.stack(bootstrapped)\n",
    "        np.save(opj(outpath, 'bootsamples', outbootfile[0]), bootstrapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_img = nb.load('/mnt/lxm2/2025/03_Tens/3group_level/all_lr+.gfeat/all_lr_mask_thr31_bin.nii.gz')\n",
    "spinal_mask = nb.load('/home/lxm/2_lxm/2025/03_Tens/LASSOPCR/group_mask_gm.nii.gz')\n",
    "\n",
    "# Load all boostraps\n",
    "bootstrapped = np.vstack(np.stack([np.load(opj(outpath, 'bootsamples', f))\n",
    "                         for f in os.listdir(opj(outpath, 'bootsamples'))\n",
    "                         if 'bootsamples' in f], axis=0))\n",
    "\n",
    "assert bootstrapped.shape[0] == nbootstraps\n",
    "\n",
    "# Get bootstraped statistics and threshold (as in nltools)\n",
    "# Zscore\n",
    "boot_z = bootstrapped.mean(axis=0)/bootstrapped.std(axis=0)\n",
    "\n",
    "# boot_z[bootstrapped.mean(axis=0) == 0] = 0\n",
    "unmask(boot_z[:brain_v], mask_img).to_filename(opj(outpath, 'brain_'+name + '_bootz.nii'))\n",
    "unmask(boot_z[brain_v:], spinal_mask).to_filename(opj(outpath, 'spinal_'+name + '_bootz.nii'))\n",
    "\n",
    "# P vals\n",
    "boot_pval =  2 * (1 - norm.cdf(np.abs(boot_z)))\n",
    "unmask(boot_pval[:brain_v], mask_img).to_filename(opj(outpath, 'brain_'+name + '_boot_pvals.nii'))\n",
    "unmask(boot_pval[brain_v:], spinal_mask).to_filename(opj(outpath, 'spinal_'+name + '_boot_pvals.nii'))\n",
    "\n",
    "def fdr(p, q=0.05):\n",
    "    s = np.sort(p)\n",
    "    nvox = p.shape[0]\n",
    "    null = np.array(range(1, nvox + 1), dtype=\"float\") * q / nvox\n",
    "    below = np.where(s <= null)[0]\n",
    "    return s[max(below)] if len(below) else -1 # p_fdr\n",
    "# FDR orrected z\n",
    "boot_z_fdr = np.where(boot_pval < fdr(boot_pval, q=0.05), boot_z, 0)\n",
    "boot_z_unc001 = np.where(boot_pval < 0.001, boot_z, 0)\n",
    "boot_z_unc005 = np.where(boot_pval < 0.005, boot_z, 0)\n",
    "boot_z_unc01 = np.where(boot_pval < 0.01, boot_z, 0)\n",
    "\n",
    "\n",
    "unmask(boot_z_fdr[:brain_v], mask_img).to_filename(opj(outpath,\n",
    "                                              'brain'+ name + '_bootz_fdr05.nii'))\n",
    "unmask(boot_z_unc001[:brain_v], mask_img).to_filename(opj(outpath,\n",
    "                                                  'brain'+ name + '_bootz_unc001.nii'))\n",
    "unmask(boot_z_unc005[:brain_v], mask_img).to_filename(opj(outpath,\n",
    "                                                    'brain'+ name + '_bootz_unc005.nii'))\n",
    "unmask(boot_z_unc01[:brain_v], mask_img).to_filename(opj(outpath,\n",
    "                                                    'brain'+ name + '_bootz_unc01.nii'))\n",
    "unmask(boot_z_fdr[brain_v:], spinal_mask).to_filename(opj(outpath,\n",
    "                                               'spianl'+ name + '_bootz_fdr05.nii'))\n",
    "unmask(boot_z_unc001[brain_v:], spinal_mask).to_filename(opj(outpath,\n",
    "                                                  'spianl'+ name + '_bootz_unc001.nii'))\n",
    "unmask(boot_z_unc005[brain_v:], spinal_mask).to_filename(opj(outpath,\n",
    "                                                    'spianl'+ name + '_bootz_unc005.nii'))\n",
    "unmask(boot_z_unc01[brain_v:], spinal_mask).to_filename(opj(outpath,\n",
    "                                                    'spianl'+ name + '_bootz_unc01.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf875bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_z_fdr_brain = np.where(boot_pval[:brain_v] < fdr(boot_pval[:brain_v], q=0.05), boot_z[:brain_v], 0)\n",
    "unmask(boot_z_fdr_brain, mask_img).to_filename(opj(outpath, 'only_brain_'+name + '_bootz_fdr05.nii'))\n",
    "boot_z_fdr_spinal = np.where(boot_pval[brain_v:] < fdr(boot_pval[brain_v:], q=0.05), boot_z[brain_v:], 0)\n",
    "unmask(boot_z_fdr_spinal, spinal_mask).to_filename(opj(outpath, 'only_spinal_'+name + '_bootz_fdr05.nii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87197462",
   "metadata": {},
   "source": [
    "# Orignial Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_sub = open('/home/lxm/2_lxm/2025/03_Tens/scripts/all.list').read().splitlines()\n",
    "# r_list = [x for x in all_sub if '_r' in x]\n",
    "# l_list = [x for x in all_sub if '_l' in x]\n",
    "# y_all_r = np.array([np.loadtxt(f'/mnt/lxm2/2025/03_Tens/0data/time_pr/{sub}_pre_pr.txt').mean() for sub in r_list])\n",
    "# y_all_l = np.array([np.loadtxt(f'/mnt/lxm2/2025/03_Tens/0data/time_pr/{sub}_pre_pr.txt').mean() for sub in l_list])\n",
    "# import numpy as np\n",
    "# import nibabel as nb\n",
    "# from nilearn.image import binarize_img,threshold_img\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# brain_mask_img  = nb.load('/mnt/lxm2/2025/03_Tens/3group_level/all_lr+.gfeat/all_lr_mask_thr31_bin.nii.gz')\n",
    "# spinal_mask_img = nb.load('/home/lxm/2_lxm/2025/03_Tens/LASSOPCR/group_mask_gm.nii.gz')\n",
    "# brain_mask  = brain_mask_img.get_fdata().astype(bool).ravel()   # (n_vox_brain,)\n",
    "# spinal_mask = spinal_mask_img.get_fdata().astype(bool).ravel()  # (n_vox_spinal,)\n",
    "\n",
    "# # 把两张掩膜长度记录下来，后面拼接用\n",
    "# len_brain, len_spinal = brain_mask.sum(), spinal_mask.sum()\n",
    "\n",
    "# def load_subject(sub_id, prefix):\n",
    "#     \"\"\"\n",
    "#     读一个受试者的脑 + 脊髓 COPE，返回 (len_brain + len_spinal,) 的 1-D 特征向量\n",
    "#     \"\"\"\n",
    "#     # 路径\n",
    "#     brain_p  = f'{prefix}/brain/{sub_id}.feat/reg_standard/stats/cope1.nii.gz'\n",
    "#     spinal_p = f'{prefix}/spinal/{sub_id}.feat/stats/cope1_template.nii.gz'\n",
    "    \n",
    "#     brain_data  = nb.load(brain_p).get_fdata().ravel()[brain_mask]     # (len_brain,)\n",
    "#     spinal_data = nb.load(spinal_p).get_fdata().ravel()[spinal_mask]   # (len_spinal,)\n",
    "    \n",
    "#     return np.concatenate((brain_data, spinal_data), axis=0)\n",
    "\n",
    "# # ── 2. 并行读取 ─────────────────────────────────────────────\n",
    "# prefix = '/mnt/lxm2/2025/03_Tens/2fst_level'\n",
    "\n",
    "# # 左右两组\n",
    "# all_data = Parallel(n_jobs=-1, backend='loky', verbose=5)(\n",
    "#     delayed(load_subject)(sub, prefix) for sub in l_list\n",
    "# )\n",
    "# test_data = Parallel(n_jobs=-1, backend='loky', verbose=5)(\n",
    "#     delayed(load_subject)(sub, prefix) for sub in r_list\n",
    "# )\n",
    "\n",
    "# all_data  = np.vstack(all_data)   # shape = (n_L, len_brain + len_spinal)\n",
    "# test_data = np.vstack(test_data)  # shape = (n_R, len_brain + len_spinal)\n",
    "\n",
    "# # ── 3. 组装后续矩阵 / 标签 / 分组 ───────────────────────────\n",
    "# # all_copes = np.vstack((all_data, test_data))\n",
    "# all_copes = np.vstack((all_data, test_data))\n",
    "# all_y     = np.hstack((y_all_l, y_all_r))\n",
    "# groups    = np.tile(np.arange(len(y_all_l)), 2)\n",
    "# from sklearn.model_selection import GroupShuffleSplit\n",
    "# group_split = GroupShuffleSplit(n_splits=2, test_size=0.33, random_state=42)\n",
    "\n",
    "# # 拆分数据集，确保同一组数据不会同时出现在训练集和测试集中\n",
    "# for train_index, test_index in group_split.split(all_copes, groups=groups):\n",
    "#     train_set = all_copes[train_index]\n",
    "#     train_y = all_y[train_index]\n",
    "#     test_set = all_copes[test_index]\n",
    "#     test_y = all_y[test_index]\n",
    "#     train_groups = groups[train_index]\n",
    "#     test_groups = groups[test_index]\n",
    "#     train_id = [r_list[x] for x in train_groups]\n",
    "#     test_id = [r_list[x] for x in test_groups]\n",
    "# outdir = '/home/lxm/2_lxm/2025/03_Tens/manuscript_FINAL/0data'\n",
    "# if not os.path.exists(outdir):\n",
    "#     os.makedirs(outdir)\n",
    "# np.save(os.path.join(outdir,'d1_train_set.npy'),train_set)\n",
    "# np.save(os.path.join(outdir,'d1_test_set.npy'),test_set)\n",
    "# np.save(os.path.join(outdir,'d1_train_y.npy'),train_y)\n",
    "# np.save(os.path.join(outdir,'d1_train_pred.npy'),y_pred)\n",
    "# np.save(os.path.join(outdir,'d1_test_y.npy'),test_y)\n",
    "# np.save(os.path.join(outdir,'d1_test_pred.npy'),lassopcr.predict(test_set))\n",
    "# np.save(os.path.join(outdir,'d2_qh_lp_pred.npy'),lassopcr.predict(qh_lp_copes))\n",
    "# np.save(os.path.join(outdir,'d2_qh_hp_pred.npy'),lassopcr.predict(qh_hp_copes))\n",
    "# np.save(os.path.join(outdir,'d2_qh_hp_y.npy'),real_hp)\n",
    "# np.save(os.path.join(outdir,'d2_qh_lp_y.npy'),real_lp)\n",
    "# np.save(os.path.join(outdir,'d1_train_group.npy'),train_groups)\n",
    "# np.save(os.path.join(outdir,'d1_test_group.npy'),test_groups)\n",
    "# np.save(os.path.join(outdir,'d2_qh_lp_pred.npy'), qh_lp_copes)\n",
    "# np.save(os.path.join(outdir,'d2_qh_hp_pred.npy'), qh_hp_copes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
